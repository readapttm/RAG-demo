{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bafba78a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ThomasMasters\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableParallel\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains.query_constructor.base import AttributeInfo\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "\n",
    "from operator import itemgetter\n",
    "import os\n",
    "import gradio as gr\n",
    "import getpass\n",
    "from typing import Generator\n",
    "\n",
    "from utility_functions import build_vectorstore, filter_context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b161af71",
   "metadata": {},
   "source": [
    "Import OpenAI key (or whichever LLM service you prefer, simply replace the llm object definition below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47383f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6451de96",
   "metadata": {},
   "source": [
    "Set token limit to reduce processing costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dda2497c",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_limit = 4000\n",
    "document_limit = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd309d8",
   "metadata": {},
   "source": [
    "Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7c4614b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gpt-4o\"\n",
    "model = ChatOpenAI(\n",
    "    model=model_name,\n",
    "    temperature=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58cb4320",
   "metadata": {},
   "source": [
    "Instantiate vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "651a3214",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ThomasMasters\\OneDrive - Re Adapt Data Science Limited\\Documents\\Blog\\Agent topics\\RAG\\rag-demo\\utility_functions.py:33: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  db = Chroma(persist_directory=\"./chroma_db\", embedding_function=OpenAIEmbeddings())\n"
     ]
    }
   ],
   "source": [
    "db = build_vectorstore(doc_folder = 'pdf_data/', documents = document_limit, rebuild = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2766da3",
   "metadata": {},
   "source": [
    "Create a self-querying retriever to filter on metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd22c1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define metadata - can add more fields as needed\n",
    "metadata_field_info = [\n",
    "    AttributeInfo(\n",
    "        name=\"year\",\n",
    "        description=\"The year of the report\",\n",
    "        type=\"float\",\n",
    "    )\n",
    "]\n",
    "\n",
    "document_content_description = \"Brief summary of a report\"\n",
    "\n",
    "# Define the retriever\n",
    "retriever = SelfQueryRetriever.from_llm(\n",
    "    ChatOpenAI(temperature=0),\n",
    "    db,\n",
    "    document_content_description,\n",
    "    metadata_field_info,\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec7e97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the retriever\n",
    "#retriever.invoke(\"What is the key to success in investing?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f136bd7",
   "metadata": {},
   "source": [
    "Define model prompt and runnable chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f37201cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Answer the question based only on the following context:\n",
    "            {context}\n",
    "\n",
    "            Question: {question}\n",
    "            \"\"\"\n",
    "\n",
    "# Create the prompt\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "## Create output dictionary\n",
    "answer = ({\n",
    "                \"response\": prompt | model,\n",
    "                \"context\": itemgetter(\"context\"),\n",
    "            }\n",
    "    )\n",
    "\n",
    "# Build the chain from components\n",
    "chain = (\n",
    "            RunnableParallel({\n",
    "                \"context\": itemgetter(\"question\") | retriever,\n",
    "                \"question\": itemgetter(\"question\")\n",
    "            })  \n",
    "            | {\n",
    "                \"context\": lambda x: filter_context(x[\"context\"], token_limit, model_name),\n",
    "                \"question\": itemgetter(\"question\")\n",
    "            }\n",
    "            | answer\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d56d16f",
   "metadata": {},
   "source": [
    "Function to stream output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5cb84ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_model_output(question: str) -> Generator[str]:\n",
    "\n",
    "    \"\"\"\n",
    "    Generator function to stream model output\n",
    "\n",
    "    Args:\n",
    "        question (str): user question to the model\n",
    "\n",
    "    Yields:\n",
    "        output (str): tokens from the model response\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    stream_gen = chain.stream({\"question\":question})    \n",
    "\n",
    "    output = ''\n",
    "\n",
    "    for s in stream_gen:\n",
    "\n",
    "        if 'response' in s.keys():\n",
    "            output += s['response'].content\n",
    "            yield output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19b7109",
   "metadata": {},
   "source": [
    "Launch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2da3872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo = gr.Interface(fn = stream_model_output, \n",
    "                    inputs = [gr.Text(label=\"Ask your question here:\")],\n",
    "                    outputs = [gr.Text(label=\"Answer\")]) \n",
    "\n",
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
